base_url: "http://localhost:8000/v1"
model: "llama-3.1-8b-w4a16"
api_key_env: "VLLM_API_KEY"
timeout_s: 60
seed: 42
temperature:
  synth: 0.7
judge:
  enabled: false
  temperature: 0.0
  max_tokens: 1024
limits:
  max_questions_per_window: 8
  synth_max_tokens: 4000
window:
  max_pages: 2
  max_chars: 12000
  max_tokens: 1800
wh_targets:
  what: 0.35
  which: 0.15
  who: 0.02
  when: 0.05
  where: 0.05
  why: 0.15
  how: 0.15
  how_many: 0.04
  how_much: 0.04
  aux: 0.05
banned_openings: ["what is ", "whatâ€™s "]
write_candidates: true
